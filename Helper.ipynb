{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Helper.ipynb","provenance":[],"collapsed_sections":["YzFkgtw6-YXM","grWVvnwhHhxH","NnXpCIuZHq8Y","eomee3oU4W5G","FENhJgxZ4Z7k"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"YzFkgtw6-YXM"},"source":["# Importamos librerías y conectamos al drive"]},{"cell_type":"code","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-09-19T13:23:07.646957Z","iopub.execute_input":"2021-09-19T13:23:07.647699Z","iopub.status.idle":"2021-09-19T13:23:07.658286Z","shell.execute_reply.started":"2021-09-19T13:23:07.647662Z","shell.execute_reply":"2021-09-19T13:23:07.657477Z"},"trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"zoXE2beFWo_h","executionInfo":{"status":"ok","timestamp":1633398124052,"user_tz":180,"elapsed":3109,"user":{"displayName":"Maria Teresita Laguinge","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16772265715389943262"}},"outputId":"e707783b-d18e-4b7a-8ef9-fb217318f1f3"},"source":["import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","import nltk\n","from nltk.tokenize import word_tokenize \n","from   nltk.tokenize import TreebankWordTokenizer \n","from   nltk.stem     import SnowballStemmer, WordNetLemmatizer #PorterStemmer\n","from   nltk.corpus   import stopwords #lista de stop words\n","\n","nltk.download('wordnet') #me bajo los datos para poder usarlos\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","\n","tokenizer  = TreebankWordTokenizer()\n","stemmer    = SnowballStemmer(\"english\") #PorterStemmer\n","lemmatizer = WordNetLemmatizer()\n","\n"," #Importo los vectorizadores\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n","from collections import Counter\n","import pickle #para guardado de datos\n","from sklearn.naive_bayes             import MultinomialNB\n","import os\n","!pip install varname\n","from varname import nameof #para obtener el nombre de variables\n","from sklearn.model_selection import cross_val_score\n","\n","# Imports para creación del Modelo:\n","import tensorflow as tf\n","from tensorflow.keras.layers import Dense\n","from tensorflow.keras.optimizers import SGD\n","from tensorflow.keras.models import Sequential\n","from keras import utils as np_utils\n","\n","# Imports para la transformación de Labels:\n","from sklearn.preprocessing import OneHotEncoder\n","from sklearn.metrics import precision_score,recall_score,f1_score,roc_auc_score, make_scorer,confusion_matrix\n","from sklearn.model_selection import cross_validate"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","Requirement already satisfied: varname in /usr/local/lib/python3.7/dist-packages (0.8.1)\n","Requirement already satisfied: asttokens<3.0.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from varname) (2.0.5)\n","Requirement already satisfied: pure_eval<1.0.0 in /usr/local/lib/python3.7/dist-packages (from varname) (0.2.1)\n","Requirement already satisfied: executing in /usr/local/lib/python3.7/dist-packages (from varname) (0.8.2)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from asttokens<3.0.0,>=2.0.0->varname) (1.15.0)\n"]}]},{"cell_type":"markdown","metadata":{"id":"grWVvnwhHhxH"},"source":["#Guardado y Cargado de archivos"]},{"cell_type":"code","metadata":{"id":"w817tkKKHkYJ"},"source":["#guardado de archivos\n","def save_txt(file,name,path='/content/drive/Shareddrives/Redes neuronales/TP1/data_results/'):\n","  path_name = f'{path}{name}.txt'\n","  with open(path_name, 'wb') as fp:\n","    pickle.dump(file, fp)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zFz8dIyxHuDh"},"source":["#cargar archivos guardados\n","def open_txt(file,path='/content/drive/Shareddrives/Redes neuronales/TP1/data/'):\n","  name = f'{path}{file}.txt'\n","  with open(name, 'rb') as fp:\n","    file = pickle.load(fp)\n","  return file"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NnXpCIuZHq8Y"},"source":["#Funciones de preprocesamiento de texto"]},{"cell_type":"code","metadata":{"id":"hUkShxpTHq8Z"},"source":["#procesamiento de todos los articulos:\n","def procesamiento_textos(text, text_name,is_lc=True,is_lem=True,is_stop=True,is_stem=True,is_alpha=True,path='/content/drive/Shareddrives/Redes neuronales/TP1/data_results/'):\n","  text_filtrado = list()\n","  text_filtrado = [\" \".join(procesamiento_texto(x,index,is_lc,is_lem,is_stop,is_stem,is_alpha)) for index,x in enumerate(text)]\n","  \n","  #nombre para guardado de archivo\n","  name = f'{text_name}_filtrado'#nameof(text)\n","  name += ('_lc' if is_lc==True else '')\n","  name += ('_lem' if is_lem==True else '')\n","  name += ('_stop' if is_stop==True else '')\n","  name += ('_stem' if is_stem==True else '')\n","  name += ('_alpha' if is_alpha==True else '')\n","\n","  save_txt(text_filtrado,name,path) #guardamos textos procesado\n","  return text_filtrado"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ghs0yatsIAgj"},"source":["#procesamiento de cada texto\n","def procesamiento_texto(text,index,is_lc=True,is_lem=True,is_stop=True,is_stem=True,is_alpha=True):\n","  # if index%100 == 0:\n","  #   print(index)\n","  text_lc = [text.lower() if is_lc==True else text] #all text in lower case\n","  tok = word_tokenize(text_lc[0]) #divide en palabras, lista de strings\n","  lem = [[lemmatizer.lemmatize(x,pos='v') for x in tok] if is_lem==True else tok] #pos='v' verbo, 'n':noun, 'a':adjetivo\n","  stop = [[x for x in lem[0] if x not in stopwords.words('english')] if is_stop==True else lem[0]] #elimina stop words,ejemplo: prepos, articulos\n","  stem = [[stemmer.stem(x) for x in stop[0]] if is_stem==True else stop[0]] #va a la raiz de las palabras\n","  alpha = [[x for x in stem[0] if x.isalpha()] if is_alpha==True else stem[0]] #elimino lo que no son palabras, ej:comas,puntos\n","  \n","  return alpha[0]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9mLVBvXwIP_9"},"source":["#Funciones NBMN"]},{"cell_type":"code","metadata":{"id":"7zaqZFWWHEmE"},"source":["def NBMN_crossval_score(train,labels_train,tf_idf_=True,alpha_ = 1,min_df_=0.000001, max_df_=0.8, ngram_range_ = (1,2),cross_sets = 5):\n","\n","  CV = (TfidfVectorizer if tf_idf_ is True else CountVectorizer)(min_df=min_df_, max_df=max_df_, ngram_range = ngram_range_)\n","  cv_train = CV.fit_transform(train)\n","\n","  clf = MultinomialNB(alpha=alpha_, class_prior=None, fit_prior=False) #construyo modelo\n","\n","  scores = cross_val_score(clf, cv_train, labels_train, cv=cross_sets)\n","  score = scores.mean()\n","  \n","  return score"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ej0xVgoWGVrB"},"source":["def crossval_score_parameters_preprocessing(train,labels_train,text_name='train_valid',is_lc_=True,is_lem_=True,is_stop_=True,is_stem_=True,is_alpha_=True,tf_idf_=True,alpha_ = 1,min_df_=0.000001, max_df_=0.8, ngram_range_ = (1,2),cross_sets = 5,path_='/content/drive/Shareddrives/Redes neuronales/TP1/data_results/'):\n","  print(\"preprocessing....\")\n","  text_train_filtrado = procesamiento_textos(train,text_name,is_lc=is_lc_,is_lem=is_lem_,is_stop=is_stop_,is_stem=is_stem_,is_alpha=is_alpha_,path=path_)\n","  print(\"evaluating model....\")\n","  score = NBMN_crossval_score(text_train_filtrado,labels_train,tf_idf_,alpha_,min_df_, max_df_, ngram_range_ ,cross_sets)\n","  return score"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6A_CCw1UAxEq"},"source":["def crossval_score_parameters_preprocessing_loaded(train,labels_train,text_name='train_valid',is_lc_=True,is_lem_=True,is_stop_=True,is_stem_=True,is_alpha_=True,tf_idf_=True,alpha_ = 1,min_df_=0.000001, max_df_=0.8, ngram_range_ = (1,2),cross_sets = 5,path='/content/drive/Shareddrives/Redes neuronales/TP1/data_results/'):\n","  print(\"loading preprocessed text....\")\n","  #nombre para guardado de archivo\n","  name = f'{text_name}_filtrado'#nameof(text)\n","  name += ('_lc' if is_lc_==True else '')\n","  name += ('_lem' if is_lem_==True else '')\n","  name += ('_stop' if is_stop_==True else '')\n","  name += ('_stem' if is_stem_==True else '')\n","  name += ('_alpha' if is_alpha_==True else '')\n","\n","  text_train_filtrado = open_txt(name,path)\n","  print(\"evaluating model....\")\n","  score = NBMN_crossval_score(text_train_filtrado,labels_train,tf_idf_,alpha_,min_df_, max_df_, ngram_range_,cross_sets)\n","  return score"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yI16q-TbIi5X"},"source":["##ver si esta\n","def run_sklearn(train_,labels_train,valid_,labels_valid,hp,test=False):\n","\n","  train = train_.copy()\n","  valid = valid_.copy()\n","  CV = (TfidfVectorizer if hp['tf_idf'] is True else CountVectorizer)(min_df=hp['min_df'], max_df=hp['max_df'], ngram_range = (1,2))\n","  cv_train = CV.fit_transform(train)\n","  cv_valid = CV.transform(valid)\n","  \n","  clf = MultinomialNB(alpha=hp['alpha'], class_prior=None, fit_prior=False) #construyo modelo\n","\n","  print('training model...')\n","  clf.fit(cv_train,labels_train)\n","\n","  print('evaluating hyperparameters...')\n","  score = clf.score(cv_valid, labels_valid)\n","\n","  return score"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"z66eZkD2CVPM"},"source":["def run_sklearn_cross_val(train_,labels_train,hp,cross_sets = 5):\n","\n","  train = train_.copy()\n","  CV = (TfidfVectorizer if hp['tf_idf'] is True else CountVectorizer)(min_df=hp['min_df'], max_df=hp['max_df'], ngram_range = (1,2))\n","  cv_train = CV.fit_transform(train)\n","  \n","  clf = MultinomialNB(alpha=hp['alpha'], class_prior=None, fit_prior=False) #construyo modelo\n","\n","  print('evaluating hyperparameters...')\n","\n","  scores = cross_val_score(clf, cv_train, labels_train, cv=cross_sets)\n","  score = scores.mean()\n","\n","  return score"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Hcd2-678KYD0"},"source":["def run_NBMN_test(train_,labels_train,valid_,labels_valid,text_test,tf_idf_=True,alpha_ = 1,min_df_=0.000001, max_df_=0.8, ngram_range_ = (1,2)):\n","\n","  train = train_.copy()\n","  valid = valid_.copy()\n","  CV = (TfidfVectorizer if tf_idf_ is True else CountVectorizer)(min_df=min_df_, max_df=max_df_, ngram_range = (1,2))\n","  cv_train = CV.fit_transform(train)\n","\n","  cv_valid = CV.transform(valid)\n","  \n","  clf = MultinomialNB(alpha=alpha_, class_prior=None, fit_prior=False) #construyo modelo\n","\n","  clf.fit(cv_train,labels_train)\n","  score = clf.score(cv_valid, labels_valid)\n","\n","  cv_test = CV.transform(text_test)\n","  test_labels = clf.predict(cv_test)\n","\n","  df_test = pd.DataFrame(data=test_labels, columns=[\"pred_labels\"],)\n","  df_test.index.names = [\"pairID\"]\n","\n","  df_test.to_csv(\"submission.csv\")\n","\n","  return df_test"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eomee3oU4W5G"},"source":["##Obtención de métricas"]},{"cell_type":"code","metadata":{"id":"MsS512nQ4ZdO"},"source":["#para obtener todas las métricas\n","def obtain_metrics(train_,labels_train,tf_idf_=True,alpha_ = 1,min_df_=0.000001, max_df_=0.8, ngram_range_ = (1,2)):\n","\n","  train = train_.copy()\n","\n","  CV = (TfidfVectorizer if tf_idf_ is True else CountVectorizer)(min_df=min_df_, max_df=max_df_, ngram_range = ngram_range_)\n","  cv_train = CV.fit_transform(train)\n","  \n","  clf = MultinomialNB(alpha=alpha_, class_prior=None, fit_prior=False) #construyo modelo\n","\n","  precision_scorer = make_scorer(precision_score, average='macro')\n","  recall_scorer = make_scorer(recall_score, average='macro')\n","  f1_scorer = make_scorer(f1_score, average='macro')\n","  roc_auc_scorer = make_scorer(roc_auc_score, multi_class='ovr',needs_proba=True) #ovr\n","\n","  secondary_metrics = ['accuracy',precision_scorer, recall_scorer, f1_scorer,roc_auc_scorer]\n","  metrics_results = pd.DataFrame(columns=[\"Accuracy\",\"Precision\",\"Recall\",\"F1 score\",\"ROC_AUC\"],)\n","  for i in range(len(secondary_metrics)):\n","    score_metric = cross_val_score(clf, cv_train, labels_train, cv=5,scoring=secondary_metrics[i]).mean() \n","    metrics_results[metrics_results.columns[i]] = [score_metric]\n","\n","  return metrics_results"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FENhJgxZ4Z7k"},"source":["##Predecir resultados"]},{"cell_type":"code","metadata":{"id":"-nj87xUL4fUn"},"source":["#para predecir los labels\n","def predict_label(train_,labels_train,text_test,tf_idf_=True,alpha_ = 1,min_df_=0.000001, max_df_=0.8, ngram_range_ = (1,2),scoring_metric = 'accuracy'):\n","\n","  CV = (TfidfVectorizer if tf_idf_ is True else CountVectorizer)(min_df=min_df_, max_df=max_df_, ngram_range = ngram_range_)\n","  cv_train = CV.fit_transform(train_)\n","  cv_test = CV.transform(text_test)\n","    \n","  clf = MultinomialNB(alpha=alpha_, class_prior=None, fit_prior=False) #construyo modelo\n","\n","  results = cross_validate(clf,cv_train,labels_train, cv=5,scoring=scoring_metric, return_estimator = True)\n","\n","  df_test = pd.DataFrame(columns=[\"1\",\"2\",\"3\",\"4\",\"5\"],)\n","  df_test.index.names = [\"pairID\"]\n","  models = results['estimator']\n","  index = 0\n","  for model in models:\n","    y_pred = model.predict(cv_test)\n","    df_test[df_test.columns[index]] = y_pred\n","    index += 1\n","  df_test['pred_labels'] = df_test.mode(axis=1)[0]\n","  \n","  df_test['pred_labels'].to_csv(\"submission.csv\") #guardado de labels predichos\n","\n","  return df_test"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GdpfvjG4CQFl"},"source":["# Funciones MLP"]},{"cell_type":"code","metadata":{"id":"9hQy30GFCR1m"},"source":["def count_vectorizer_MLP(train, valid, min_df_=0.000001, max_df_=0.8, ngram_range_ = (1,2)):\n","  cv = CountVectorizer(min_df=min_df_, max_df=max_df_, ngram_range = ngram_range_)\n","  cv_train = cv.fit_transform(train)\n","  cv_valid = cv.transform(valid)\n","  return cv_train, cv_valid"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oReO96ziE1_-"},"source":["# Fiteamos el OneHot\n","def labels_transform(labels_train, labels_val):\n","  ohe = OneHotEncoder()\n","  ohe.fit(np.array(labels_train).reshape(-1,1))\n","  labels_train_enc = ohe.transform(np.array(labels_train).reshape(-1,1))\n","  labels_valid_enc = ohe.transform(np.array(labels_val).reshape(-1,1))\n","  return labels_train_enc, labels_valid_enc"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wTFWVBVjE3rX"},"source":["# Se usa un generador de datos:\n","class DataGenerator(np_utils.all_utils.Sequence):\n","  'Generates data for Keras'\n","  def __init__(self, X_data, labels, batch_size=32, n_channels=1, shuffle=True):\n","    'Initialization'\n","    # Propiedades:\n","    self.batch_size = batch_size\n","    self.labels = labels\n","    self.X_data = X_data\n","    self.n_channels = n_channels\n","    self.shuffle = shuffle\n","    self.on_epoch_end()\n","\n","  def __len__(self):\n","    'Denotes the number of batches per epoch'\n","    samples_per_epoch = self.X_data.shape[0]\n","    return int(np.floor(samples_per_epoch / self.batch_size))\n","\n","\n","  def __getitem__(self, counter):\n","    'Generate one batch of data'\n","    # Generate indexes of the batch\n","    indexes = self.indexes[counter*self.batch_size:(counter+1)*self.batch_size]\n","    # Generate data\n","    X, y = self.__data_generation(indexes)\n","    return X, y\n","\n","\n","  def on_epoch_end(self):\n","    # Este metodo será llamado al principio de la clase y una vez al final de cada epoch.\n","    'Updates indexes after each epoch'\n","    self.indexes = np.arange(self.X_data.shape[0]) # Acá declaro mis indexes\n","    if self.shuffle == True:\n","      # Shuffleo mis indexes\n","      np.random.shuffle(self.indexes)\n","\n","\n","  def __data_generation(self, indexes):\n","    # Produce el batche de datos en si.\n","    'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n","    # Initialization\n","    X_batch = self.X_data[indexes,:].todense()\n","    y_batch = self.labels[indexes,:].todense()\n","    return np.array(X_batch), np.array(y_batch)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Jh_TpIbUWeR9"},"source":["def MLP_model(cant_capas,cv_train, cant_neuronas = [3], activation_func = [\"softmax\"], learning_rate = 0.01, alpha = None, loss = \"categorical_crossentropy\", metrics = [\"accuracy\"]):\n","  model = Sequential()\n","  max_capas = 0\n","  if len(activation_func) == cant_capas and len(cant_neuronas) == cant_capas:\n","    while cant_capas > max_capas:\n","    # Voy agregando capas, a medida que se necesite\n","      if max_capas == 0:\n","        if activation_func[max_capas] == \"LeakyReLU\":\n","          layer1 = tf.keras.layers.Dense(cant_neuronas[max_capas], activation= tf.keras.layers.LeakyReLU(alpha=alpha[max_capas]),  input_shape = (cv_train.shape[1], ))\n","          #model = tf.keras.Sequential([layer1])\n","          model.add(layer1)\n","          #print('entre al LeakyRelu', max_capas)\n","        else:\n","          #print('entre al else', max_capas)\n","          model.add(Dense(cant_neuronas[max_capas], activation = activation_func[max_capas], input_shape = (cv_train.shape[1], )))\n","      else:\n","        #print('entra 2')\n","        if activation_func[max_capas] == \"LeakyReLU\":\n","          #print('volvio a entrar', max_capas)\n","          layer1 = tf.keras.layers.Dense(cant_neuronas[max_capas], activation= tf.keras.layers.LeakyReLU(alpha=alpha[max_capas]))\n","          model.add(layer1)  \n","        else:\n","          #print('me hace bien', max_capas)\n","          model.add(Dense(cant_neuronas[max_capas], activation = activation_func[max_capas]))\n","        # Esta capa toma como el input el output de la primera capa oculta.\n","      max_capas += 1\n","  else:\n","    print(\"No coinciden la cantidad de capas con las dimensiones de 'cant_neuronas' y  'activation_func'\")\n","\n","  model.compile(optimizer= SGD(learning_rate= learning_rate), loss = loss, metrics = metrics)\n","\n","  return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ligFKx_Ew2_o"},"source":["def crear_MLP_model(text_train, text_val, cant_capas, min_df_=0.000001, max_df_=0.8, ngram_range_ = (1,2), cant_neuronas = [3], activation_func = [\"softmax\"], learning_rate = 0.01, alpha = None, loss = \"categorical_crossentropy\", metrics = [\"accuracy\"]):\n","  # Conversión a sparse matrix:\n","  cv_train, cv_valid = count_vectorizer_MLP(text_train,text_val, min_df_, max_df_, ngram_range_)\n","\n","  # Creación y compilación de Modelo deseado:\n","  model = MLP_model(cant_capas, cv_train, cant_neuronas= cant_neuronas, activation_func= activation_func, learning_rate= learning_rate, alpha= alpha, loss = loss, metrics= metrics)\n","\n","  # El return:\n","  return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pk7xpmosw4hR"},"source":["def entrenar_MLP_model(model,text_train, labels_train, text_val, labels_val, batch_size, batch_val, min_df_=0.000001, max_df_=0.8, ngram_range_ = (1,2), verbose = 1, epochs = 10):\n","  # Conversión a sparse matrix:\n","  cv_train, cv_valid = count_vectorizer_MLP(text_train,text_val, min_df_, max_df_, ngram_range_)\n","  labels_train_enc, labels_valid_enc = labels_transform(labels_train, labels_val)\n","  print(cv_train.shape)\n","  print(cv_valid.shape)\n","\n","  # Entrenamiento del modelo:\n","  print('training model....')\n","  model.fit(DataGenerator(cv_train, labels_train_enc, batch_size), validation_data=DataGenerator(cv_valid,labels_valid_enc,batch_val),verbose= verbose, epochs = epochs)\n","\n","  return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5vPqDHF-SqTg"},"source":["def count_vector_test(train, test, min_df_=0.000001, max_df_=0.8, ngram_range_ = (1,1)):\n","  cv = CountVectorizer(min_df=min_df_, max_df=max_df_, ngram_range = ngram_range_)\n","  cv_train = cv.fit_transform(train)\n","  cv_test = cv.transform(test)\n","  return cv_test"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Kt5Na6lsNJT8"},"source":["def convert_pd(datos, labels):\n","  len_datos = datos.shape[0]\n","  list_predict = []\n","  for index in range(len_datos):\n","    list_predict.append([labels[datos[index]]])\n","  submission = pd.DataFrame(data=list_predict, columns=[\"pred_labels\"])\n","  submission.index.names = [\"pairID\"]\n","  return submission"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kNiZ9MprNKOM"},"source":["def transform_string_num(list_labels):\n","  arg = np.zeros(len(list_labels))\n","  for i in range(len(arg)):\n","    if list_labels[i] == 'contradiction':\n","      arg[i] = int(0)\n","    elif list_labels[i] == 'entailment':\n","      arg[i] = int(1)\n","    elif list_labels[i] == 'neutral':\n","      arg[i] = int(2)\n","  return arg"],"execution_count":null,"outputs":[]}]}